<!DOCTYPE html>
<html lang=" en">

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination | Reflected Intelligence</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination" />
<meta name="author" content="Evan Volgas" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="As AI systems increasingly rely on self-examination capabilities, attackers are finding ways to manipulate reflection mechanisms themselves. This post explores emerging security vulnerabilities in reflective AI and strategies to defend against them." />
<meta property="og:description" content="As AI systems increasingly rely on self-examination capabilities, attackers are finding ways to manipulate reflection mechanisms themselves. This post explores emerging security vulnerabilities in reflective AI and strategies to defend against them." />
<link rel="canonical" href="https://reflectedintelligence.com/2025/08/11/adversarial-reflection-how-attackers-can-manipulate-ai-self-examination/" />
<meta property="og:url" content="https://reflectedintelligence.com/2025/08/11/adversarial-reflection-how-attackers-can-manipulate-ai-self-examination/" />
<meta property="og:site_name" content="Reflected Intelligence" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-11T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Evan Volgas"},"dateModified":"2025-08-11T00:00:00-05:00","datePublished":"2025-08-11T00:00:00-05:00","description":"As AI systems increasingly rely on self-examination capabilities, attackers are finding ways to manipulate reflection mechanisms themselves. This post explores emerging security vulnerabilities in reflective AI and strategies to defend against them.","headline":"Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination","mainEntityOfPage":{"@type":"WebPage","@id":"https://reflectedintelligence.com/2025/08/11/adversarial-reflection-how-attackers-can-manipulate-ai-self-examination/"},"url":"https://reflectedintelligence.com/2025/08/11/adversarial-reflection-how-attackers-can-manipulate-ai-self-examination/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/%20/assets/css/main.css">
  <link rel="stylesheet" href="/%20/assets/css/custom.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"
    rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500;600&display=swap" rel="stylesheet">
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Favicon -->
  <link rel="shortcut icon" href="/%20/assets/favicon/favicon.ico">
  <link rel="icon" href="/%20/assets/favicon/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/%20/assets/favicon/favicon.ico"><link type="application/atom+xml" rel="alternate" href="https://reflectedintelligence.com/feed.xml" title="Reflected Intelligence" /><!-- Dark mode toggle script -->
  <script>
    function toggleDarkMode() {
      document.body.classList.toggle('dark-mode');
      localStorage.setItem('darkMode', document.body.classList.contains('dark-mode'));
    }

    document.addEventListener('DOMContentLoaded', function () {
      if (localStorage.getItem('darkMode') === 'true') {
        document.body.classList.add('dark-mode');
      }
    });
  </script>
</head>

<body>
  <header class="site-header">
  <div class="wrapper">
    <div class="header-content">
      <a class="site-title" rel="author" href="/%20/">
        <span class="title-text">Reflected Intelligence</span>
      </a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path
                d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,9,0,8.335,0,7.516c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516c0,0.82-0.665,1.483-1.484,1.483H1.484C0.665,15,0,14.335,0,13.516c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">
            <i class="fas fa-tags"></i> Categories
          </a>
          <a class="page-link" href="#" id="search-toggle">
            <i class="fas fa-search"></i>
          </a>
          <button class="dark-mode-toggle" onclick="toggleDarkMode()">
            <i class="fas fa-moon"></i>
          </button>
        </div>
      </nav>
    </div>

    <div id="search-container" class="search-container">
      <form class="search-form" action="/search/" method="get">
        <input type="text" class="search-input" name="q" placeholder="Search the blog...">
        <button type="submit" class="search-button">
          <i class="fas fa-search"></i>
        </button>
      </form>
    </div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const searchToggle = document.getElementById('search-toggle');
      const searchContainer = document.getElementById('search-container');

      searchToggle.addEventListener('click', function (e) {
        e.preventDefault();
        if (searchContainer.style.display === 'none') {
          searchContainer.style.display = 'block';
          searchContainer.querySelector('input').focus();
        } else {
          searchContainer.style.display = 'none';
        }
      });
    });
  </script>
</header>

  

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-08-11T00:00:00-05:00" itemprop="datePublished">Aug 11, 2025
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Evan Volgas</span></span>
      
      <span class="reading-time"><i class="far fa-clock"></i> 6 min read</span><br>
      <span class="post-categories">
        <i class="fas fa-tags"></i> Categories:
        
        <a class="category-link" href="/categories/#AI">AI</a>, 
        
        <a class="category-link" href="/categories/#Reflection">Reflection</a>, 
        
        <a class="category-link" href="/categories/#Security">Security</a>, 
        
        <a class="category-link" href="/categories/#Adversarial Attacks">Adversarial Attacks</a>
        
      </span></p>

    
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="adversarial-reflection-how-attackers-can-manipulate-ai-self-examination">Adversarial Reflection: How Attackers Can Manipulate AI Self-Examination</h1>

<p>Our exploration of reflection in AI has covered everything from <a href="/2025/08/04/training-for-reflection-how-to-build-self-examining-ai-systems/">implementation strategies</a> to <a href="/2025/07/14/the-limits-of-reflection-where-ai-self-examination-fails/">fundamental limitations</a>. However, a critical aspect demands urgent attention: as reflection becomes central to AI safety, it also introduces novel attack vectors. Attackers are now targeting these very mechanisms designed to make systems safer.</p>

<h2 id="the-reflection-attack-surface">The Reflection Attack Surface</h2>

<p>Reflection creates new vulnerabilities by introducing additional processing layers that can be independently manipulated. Stanford’s Cybersecurity Lab demonstrated that systems using explicit reflection have a 34% larger attack surface compared to non-reflective counterparts, with particularly concerning implications for high-stakes applications.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p>

<p>These vulnerabilities exist because reflection mechanisms maintain separate evaluation criteria from primary generation systems. As Microsoft Research notes, “The very separation that makes reflection effective for error detection creates a security boundary that can be independently compromised.”<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<h2 id="inducing-reflection-hallucinations">Inducing Reflection Hallucinations</h2>

<p>Perhaps the most concerning attack vector is what security researchers call “reflection hallucination injection.” These attacks trick a system’s reflection mechanisms into fabricating problems that don’t exist or overlooking real issues.</p>

<p>Princeton’s Advanced Security Group demonstrated that adversarially crafted inputs could induce hallucinated errors in 76% of tested reflection systems, often leading to one of two harmful outcomes:<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p>

<ol>
  <li><strong>False Rejection</strong>: The system incorrectly identifies legitimate outputs as problematic</li>
  <li><strong>Distraction Flooding</strong>: The system becomes overwhelmed with fabricated minor issues while missing critical flaws</li>
</ol>

<p>The most effective attacks exploit what researchers call “reflection trigger phrases”—specific patterns that activate reflection mechanisms but with manipulated evaluation criteria. As one researcher explained, “We can effectively hijack the system’s self-criticism framework by embedding specific linguistic patterns that prime reflection toward fabricated concerns.”<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p>

<h2 id="reflection-blindness-attacks">Reflection Blindness Attacks</h2>

<p>The inverse attack—inducing “reflection blindness”—aims to prevent systems from detecting genuine problems. Carnegie Mellon’s AI Security Team documented how carefully crafted adversarial examples can create specific blind spots in reflection mechanisms, with success rates of 52-68% depending on the architecture.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p>

<p>These attacks are particularly effective against transformer-based systems. By exploiting attention mechanisms, attackers can create what researchers call “reflection shadows”—areas of reasoning that become invisible to the system’s self-examination process. Meta AI’s security research demonstrated that even advanced systems could be manipulated to overlook contradictions when specific distractor patterns were present in the input.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup></p>

<h2 id="reflection-poisoning-across-architectures">Reflection Poisoning Across Architectures</h2>

<p>Different AI architectures show varying vulnerability to what researchers call “reflection poisoning”—the systematic degradation of reflection quality through adversarial inputs.</p>

<p>Berkeley’s comparative analysis found that vulnerability varies significantly by architecture type:<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup></p>

<ul>
  <li><strong>Transformer-based systems</strong>: Most vulnerable to attention manipulation (83% attack success rate)</li>
  <li><strong>Reinforcement learning systems</strong>: Moderately vulnerable through reward confusion (61% success rate)</li>
  <li><strong>Neurosymbolic systems</strong>: Most resistant overall (37% success rate) but catastrophically vulnerable to specific symbolic manipulation attacks</li>
</ul>

<p>The DeepMind security team found that hybrid architectures often inherit the vulnerabilities of both component types rather than mitigating them. Their analysis showed that “reflection transfer attacks” could successfully migrate from vulnerable components to otherwise secured modules in 71% of tested hybrid systems.<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup></p>

<h2 id="building-defensive-reflection">Building Defensive Reflection</h2>

<p>Despite these challenges, researchers are developing promising defensive strategies. MIT’s AI Security Lab demonstrated that “reflection diversity”—implementing multiple, independent reflection mechanisms with different architectural foundations—reduced successful attacks by 64% compared to single-mechanism approaches.<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">9</a></sup></p>

<p>Other effective countermeasures include:</p>

<ul>
  <li><strong>Reflection verification chains</strong>: Using secondary verification systems to validate reflection outputs</li>
  <li><strong>Adversarial reflection training</strong>: Explicitly training on attempted manipulations</li>
  <li><strong>Reflection confidence calibration</strong>: Developing uncertainty metrics for reflection quality</li>
</ul>

<p>Anthropic’s security team demonstrated that systems trained specifically to identify manipulation attempts in their reflection processes could detect 79% of reflection attacks, suggesting that reflection itself can become a security asset rather than just a vulnerability.<sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">10</a></sup></p>

<h2 id="reflection-as-a-security-tool">Reflection as a Security Tool</h2>

<p>Perhaps most intriguing is the emerging use of reflection as a security tool itself. Google’s cybersecurity division showed that specialized reflection mechanisms can detect 67% of adversarial inputs before they reach primary processing, essentially using reflection as an immune system against attacks.<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">11</a></sup></p>

<p>This approach treats reflection as a form of computational immune system, continuously monitoring for patterns that might indicate manipulation attempts. As one researcher noted, “By training reflection mechanisms to question not just outputs but inputs, we transform a potential vulnerability into a powerful defense.”<sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">12</a></sup></p>

<h2 id="toward-robust-reflective-ai">Toward Robust Reflective AI</h2>

<p>The arms race between reflection attacks and defenses highlights how security considerations must be central to reflection implementation. As our <a href="/2025/08/04/training-for-reflection-how-to-build-self-examining-ai-systems/">previous exploration of training approaches</a> emphasized, robust reflection requires thinking beyond capability to include security by design.</p>

<p>By understanding potential attack vectors and implementing appropriate defenses, we can build systems where reflection serves its intended purpose—making AI more reliable, honest, and safe—without introducing catastrophic new vulnerabilities.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ul>
  <li>
    <p><strong>Expanded Attack Surface</strong>: Systems with explicit reflection mechanisms have a 34% larger attack surface compared to non-reflective counterparts, creating novel vulnerability points.</p>
  </li>
  <li>
    <p><strong>Reflection Hallucinations</strong>: Adversarially crafted inputs can induce hallucinated errors in 76% of tested reflection systems, causing false rejections or distraction from real issues.</p>
  </li>
  <li>
    <p><strong>Architectural Vulnerabilities</strong>: Different architectures show varying susceptibility to reflection attacks—transformer-based systems are most vulnerable to attention manipulation (83% attack success), while neurosymbolic systems show more targeted weaknesses.</p>
  </li>
  <li>
    <p><strong>Defensive Approaches</strong>: “Reflection diversity” implementing multiple independent reflection mechanisms reduces successful attacks by 64%, while systems trained specifically to detect manipulation attempts can identify 79% of reflection attacks.</p>
  </li>
  <li>
    <p><strong>Security Applications</strong>: Beyond being a vulnerability, reflection can serve as a security mechanism itself, with specialized reflection systems detecting 67% of adversarial inputs before primary processing.</p>
  </li>
  <li>
    <p><strong>Design Implications</strong>: Secure reflection requires explicit consideration of potential attack vectors during system design, not just as an afterthought during deployment.</p>
  </li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://cisac.stanford.edu/publications/quantifying-attack-surface-reflection">Chen, J., &amp; Carlini, N. (2025). <em>Quantifying the Attack Surface of Reflective AI Systems</em>. Stanford Cybersecurity Lab Technical Reports.</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://www.microsoft.com/en-us/research/publication/security-boundaries-reflection-2025">Microsoft Research. (2025). <em>Security Boundaries in Self-Examining AI Systems</em>. Microsoft Research Technical Report MSR-TR-2025-11.</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://www.princeton.edu/~security/publications/reflection-hallucination-2024">Roth, M., &amp; Feamster, N. (2024). <em>Reflection Hallucination Injection Attacks Against Self-Improving AI</em>. Princeton Advanced Security Group Technical Reports.</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://ieeexplore.ieee.org/document/10139267">Lee, S., &amp; Goldstein, T. (2025). <em>Linguistic Triggers for Manipulating AI Reflection Mechanisms</em>. In Proceedings of the 38th IEEE Symposium on Security and Privacy, 426-441.</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p><a href="https://www.cylab.cmu.edu/research/adversarial-blindness-2024">Carnegie Mellon AI Security Team. (2024). <em>Adversarial Blindness in Reflective AI Systems</em>. CMU CyLab Security and Privacy Institute.</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p><a href="https://ai.meta.com/research/publications/attention-manipulation-reflection/">Meta AI Security Research. (2025). <em>Attention Manipulation Attacks Against Reflective Transformers</em>. Meta AI Research Publications.</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p><a href="https://bair.berkeley.edu/blog/2025/reflection-vulnerability-analysis">Song, C., &amp; Wagner, D. (2025). <em>Comparative Vulnerability Analysis of Reflection Mechanisms Across AI Architectures</em>. Berkeley Artificial Intelligence Research.</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p><a href="https://deepmind.google/blog/reflection-transfer-attacks-2024/">DeepMind Security Team. (2024). <em>Reflection Transfer Attacks in Hybrid AI Architectures</em>. DeepMind Research Blog.</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p><a href="https://www.csail.mit.edu/research/reflection-diversity-defense">MIT AI Security Lab. (2025). <em>Reflection Diversity as a Defense Against Adversarial Manipulation</em>. MIT CSAIL Technical Reports.</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p><a href="https://www.anthropic.com/research/reflection-manipulation-detection">Anthropic Security Team. (2025). <em>Training Reflection Mechanisms to Detect Their Own Manipulation</em>. Anthropic Technical Reports.</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p><a href="https://research.google/pubs/reflection-immune-system/">Google Cybersecurity Division. (2024). <em>Reflection as an Immune System: Detecting Adversarial Inputs Through Self-Examination</em>. Google Research Publications.</a> <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p><a href="https://www.usenix.org/conference/usenixsecurity25/presentation/zhao-reflection">Zhao, M., &amp; Shmatikov, V. (2025). <em>Defensive Applications of AI Reflection</em>. In Proceedings of the 2025 USENIX Security Symposium, 217-233.</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  <a class="u-url" href="/2025/08/11/adversarial-reflection-how-attackers-can-manipulate-ai-self-examination/" hidden></a>

  <div class="post-share">
    <h4>Share this post</h4>
    <div class="share-buttons">
      <a href="https://twitter.com/intent/tweet?text=Adversarial+Reflection%3A+How+Attackers+Can+Manipulate+AI+Self-Examination&url=https://reflectedintelligence.com%2F2025%2F08%2F11%2Fadversarial-reflection-how-attackers-can-manipulate-ai-self-examination%2F" target="_blank"
        title="Share on Twitter"><i class="fab fa-twitter"></i></a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://reflectedintelligence.com%2F2025%2F08%2F11%2Fadversarial-reflection-how-attackers-can-manipulate-ai-self-examination%2F&title=Adversarial+Reflection%3A+How+Attackers+Can+Manipulate+AI+Self-Examination"
        target="_blank" title="Share on LinkedIn"><i class="fab fa-linkedin"></i></a>
      <a href="https://www.facebook.com/sharer/sharer.php?u=https://reflectedintelligence.com%2F2025%2F08%2F11%2Fadversarial-reflection-how-attackers-can-manipulate-ai-self-examination%2F" target="_blank" title="Share on Facebook"><i
          class="fab fa-facebook"></i></a>
      <a href="mailto:?subject=Adversarial+Reflection%3A+How+Attackers+Can+Manipulate+AI+Self-Examination&body=Check out this article: https://reflectedintelligence.com%2F2025%2F08%2F11%2Fadversarial-reflection-how-attackers-can-manipulate-ai-self-examination%2F" title="Share via Email"><i
          class="fas fa-envelope"></i></a>
    </div>
  </div>
</article>

<div class="post-navigation">
  <div class="post-navigation-links">
    
    <a class="prev" href="/2025/08/04/training-for-reflection-how-to-build-self-examining-ai-systems/"><i class="fas fa-arrow-left"></i> Training for Reflection: How to Build Self-Examining AI Systems</a>
    
    
    <a class="next" href="/2025/08/18/emergent-reflection-self-examination-in-systems-not-explicitly-trained-to-reflect/">Emergent Reflection: Self-Examination in Systems Not Explicitly Trained to Reflect <i class="fas fa-arrow-right"></i></a>
    
  </div>
</div>

<script>
  // Add class behaviors for dynamic styling
  document.addEventListener('DOMContentLoaded', function () {
    // Convert first blockquote to a pull-quote if it's short enough
    const firstBlockquote = document.querySelector('.post-content blockquote');
    if (firstBlockquote && firstBlockquote.textContent.length < 200) {
      firstBlockquote.classList.add('pull-quote');
    }

    // Add syntax highlighting line numbers
    document.querySelectorAll('pre.highlight').forEach(function (el) {
      el.classList.add('line-numbers');
    });

    // Highlight sentences marked in markdown (==...==) by adding classes
    document.querySelectorAll('.post-content mark').forEach(function (el) {
      el.classList.add('highlight-sentence');
      if (el.textContent.trim().length > 120) {
        el.classList.add('big-highlight');
      }
    });
  });
</script>
    </div>
  </main>

  <footer class="site-footer h-card">
<data class="u-url" href="/"></data>

<div class="wrapper">
<div class="footer-col-wrapper">
<div class="footer-col footer-col-1">
<h2 class="footer-heading">Reflected Intelligence</h2>
<ul class="contact-list">
<li class="p-name">Evan Volgas</li></ul>
</div>

<div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

<div class="footer-col footer-col-3">
<p>Making AI Smarter</p>
</div>
</div>
</div>
</footer>

</body>

</html>